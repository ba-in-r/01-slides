---
title: "Unidad 5 — Agrupando Observaciones"
subtitle: "Semana 08: Clustering"
author: "Eduard F. Martínez González"
---
  
<a href="mailto:efmartinez@icesi.edu.co" style="color:black;">
<img src="pic/correo.png" alt="Email" width="20" height="20"/> efmartinez@icesi.edu.co
</a>

<a href="https://github.com/eduard-martinez" style="color:black;"> <img src="pic/github.png" alt="Qries" width="20" height="20"/> eduard-martinez</a>

<a href="https://twitter.com/emartigo" style="color:black;"> <img src="pic/twitter.jpg" alt="Qries" width="20" height="20"/> @emartigo</a>

<a href="https://eduard-martinez.github.io" style="color:black;"> <img src="pic/link.png" alt="Qries" width="20" height="20"/> https://eduard-martinez.github.io</a>

---

# Clustering jerárquico y dendrogramas

El clustering jerárquico construye una estructura en forma de árbol que muestra cómo se agrupan progresivamente las observaciones según su semejanza. La representación visual es el dendrograma: cada “hoja” corresponde a una observación; las “ramas” reflejan fusiones de grupos; y la altura de unión indica la diferencia (distancia) entre los grupos que se combinan.

## Enfoques aglomerativo y divisivo

Existen dos enfoques conceptuales. En el aglomerativo (bottom-up), se inicia con cada observación como su propio grupo y, en pasos sucesivos, se fusionan los clusters más próximos hasta obtener una sola agrupación. En el divisivo (top-down), se parte de un único gran cluster y se divide recursivamente. En la práctica docente suele emplearse principalmente el enfoque aglomerativo por su interpretabilidad y disponibilidad en paquetes estándar.

## Proximidad: métricas y criterios de enlace

La noción de “proximidad” depende de dos decisiones. Primero, la métrica de distancia; con variables en escalas distintas, se recomienda estandarizar (media 0, desviación 1) y usar, por ejemplo, distancia euclidiana. Segundo, el criterio de enlace (linkage), que define la distancia entre clusters: single (distancia mínima entre pares, propenso a “encadenar”), complete (distancia máxima, favorece clusters compactos), average (promedio de distancias) y Ward.D2 (fusiona minimizando el aumento de varianza intracluster, produciendo grupos usualmente más esféricos y balanceados). A diferencia de K-means, no se fija el número de clusters k de entrada; el dendrograma permite seleccionar k ex-post.

## Elección del número de clusters

El número de grupos se elige “cortando” horizontalmente el dendrograma a una cierta altura. Un corte alto produce menos grupos y mayor generalidad; un corte bajo produce más grupos y mayor especificidad. Saltos grandes en la altura de las fusiones sugieren fronteras naturales entre segmentos y orientan una elección prudente de k.

## Algoritmo aglomerativo: descripción operativa

Para implementar el enfoque aglomerativo, se procede usualmente de la siguiente manera. Primero, se preparan las variables; cuando se encuentran en escalas heterogéneas, se estandarizan. Luego, se calcula la matriz de distancias entre observaciones con la métrica elegida. A continuación, se inicia con n clusters (uno por observación) y, de forma iterativa, se identifican y fusionan los dos clusters más cercanos según el criterio de enlace seleccionado, actualizando después las distancias entre el nuevo cluster y los existentes. El proceso continúa hasta que queda un único cluster; el historial de fusiones define el dendrograma. Finalmente, se selecciona k cortando el dendrograma a una altura que evite fusiones “costosas”, es decir, asociadas a incrementos bruscos en la altura.

## Lectura rápida de un dendrograma

La interpretación sigue pautas simples. Las hojas representan observaciones individuales. La altura de unión indica cuán diferentes eran los grupos al fusionarse: una unión a gran altura sugiere que los grupos combinados estaban muy separados. Un corte horizontal determina el número de clusters; las ramas que permanecen separadas por debajo del corte definen los grupos resultantes. La presencia de grandes saltos verticales antes de ciertas fusiones suele señalar límites naturales entre segmentos.

## Ventajas y limitaciones

Entre las ventajas, no se requiere fijar k al inicio, se obtiene una representación jerárquica que facilita justificar segmentaciones y se dispone de una lectura multiescala de la estructura de los datos. Entre las limitaciones, su complejidad computacional puede ser mayor que la de K-means en conjuntos de datos muy grandes, y la solución es sensible tanto a las escalas de las variables como a la elección de métrica y criterio de enlace, por lo que la estandarización y la verificación de robustez resultan recomendables.

---

# Aplicación en R

Para ejemplificar el uso de K-means se trabajará con una base de notas del curso, que reúne para cada estudiante sus calificaciones en talleres, quices, parcial y midterm. Imagine un diagrama de dispersión donde en el eje X se ubica el promedio de talleres y en el eje Y el promedio de quices. Al ajustar K-means con k = 3, el algoritmo puede revelar patrones como los siguientes: Cluster 1, formado por estudiantes con desempeños altos y consistentes en trabajos y evaluaciones cortas; Cluster 2, con estudiantes que rinden mejor en actividades prácticas (talleres) pero muestran rezagos en quices; y Cluster 3, que agrupa desempeños más bajos o inestables en ambas dimensiones.

Este ejercicio permite observar cómo el algoritmo construye segmentos naturales a partir de los datos, sin categorías predefinidas, resaltando la utilidad del aprendizaje no supervisado para explorar perfiles y patrones de desempeño académico.

::: callout-tip
**Cómo usar este material:** Puedes ejecutar los _chunks_ de R directamente en el navegador gracias a **webR** (según tu `_quarto.yml`), sin instalar nada localmente.
:::

## Preparación del entorno

El propósito de este bloque es asegurar un entorno limpio y reproducible. Primero, eliminamos objetos previos que puedan interferir con el análisis. Después, cargamos los paquetes necesarios para manipulación de datos, visualización, generación de resúmenes y ejecución de `K-means`. Con esto, dejamos el entorno preparado para comenzar.

```{webr-r}
## Author:
## Date: 

## limpiar entorno
rm(list = ls())

## cargar paquetes
require(dplyr)      # Manipulación de datos 
require(ggplot2)    # Visualización de datos
require(skimr)      # Resúmenes rápidos y completos
```

## Ingesta de datos (carga desde archivo o URL)

En esta sección se realiza la ingesta de datos, es decir, la carga de la base de notas del curso que usaremos para los ejercicios de clustering. El dataset llamado `grades` incluye, para cada estudiante, su código y las calificaciones en `talleres`, `quices`, `parcial` y `midterm` (escala 0–5). Estos atributos permitirán aplicar `K-means` para identificar patrones de desempeño y segmentar a los estudiantes en grupos con perfiles similares.

Para garantizar reproducibilidad, podemos trabajar de dos maneras:  

1. **Archivo local**: leer directamente el archivo `data.csv` que acompaña a este material.  
2. **Fuente externa (opcional)**: usar una URL (por ejemplo, un repositorio en GitHub) para que cualquier estudiante pueda replicar el ejercicio sin necesidad del archivo local.

Una vez cargados los datos, listamos los objetos en memoria para confirmar que el dataset esperado (`grades`) está disponible y listo para la exploración inicial.

```{webr-r}
#| warning: false
#| message: false

## generar los datos
source("https://raw.githubusercontent.com/ba-in-r/01-slides/main/week-09/data/grades.R")

## chuequear objetos en la memoria
ls()

## atributos de grades
class(grades)
skim(grades)
```

## Selección de variables

En este paso se eligen únicamente las variables numéricas relevantes para el agrupamiento. En el caso de las notas, pueden incluirse `taller`, `quiz`, `parcial` y `midterm.` Estas variables son adecuadas para `K-means` porque están en la misma escala (0–5) y describen cuantitativamente el desempeño académico.

Para un ejemplo simple, trabajaremos con dos dimensiones (`quiz` y `parcial`) a fin de visualizar con claridad la lógica del clustering. Al reducir la base a este subconjunto se garantiza que el análisis se centre en atributos comparables y se excluyan identificadores u otras columnas que no aportan a la formación de grupos.

```{webr-r}
## filtrar variables
db <- select(.data=grades , quiz,  parcial)
```

## Escalado

Antes de aplicar `K-means` es buena práctica escalar las variables para que todas queden con media 0 y desviación estándar 1. Así se evita que alguna dimensión pese más en el cálculo de distancias euclidianas. Aunque aquí todas las notas comparten el rango 0–5, el escalado ayuda a estandarizar y comparar contribuciones.

Con `scale()` se obtiene una matriz estandarizada lista para el análisis. Luego, se usa `skim()` para comprobar rápidamente que la transformación se aplicó correctamente (medias cercanas a 0 y desviaciones cercanas a 1).

```{webr-r}
## escalar
db <- scale(db)

## check data (skim)
skim(db)

## visualicemos las variables
ggplot(data=db , aes(x=quiz , y=parcial)) +
geom_point() +
theme_basic()
```




	2.	Matriz de distancias y clustering jerárquico

	•	Usa distancia Euclidiana.
	•	Compara al menos dos métodos de enlace (por ejemplo, ward.D2 y complete).

# --- 2. Distancias y modelos ---
D  <- dist(Xz, method = "euclidean")

hc_ward     <- hclust(D, method = "ward.D2")
hc_complete <- hclust(D, method = "complete")

	3.	Dendrogramas

	•	Etiqueta con codigo.
	•	Señala, por ejemplo, k = 3 grupos (ajústalo si ves otro corte más natural).

# --- 3. Dendrogramas ---
par(mfrow = c(1,2))
plot(hc_ward, labels = df$codigo, main = "Dendrograma - Ward.D2", xlab = "", sub = "")
rect.hclust(hc_ward, k = 3, border = "red")

plot(hc_complete, labels = df$codigo, main = "Dendrograma - Complete", xlab = "", sub = "")
rect.hclust(hc_complete, k = 3, border = "red")
par(mfrow = c(1,1))

	4.	Corte del árbol y comparación con group

	•	Obtén asignaciones de cluster y compáralas con la columna group.

# --- 4. Corte y comparación ---
k <- 3
cl_w <- cutree(hc_ward, k = k)
cl_c <- cutree(hc_complete, k = k)

# Tabla de contingencia vs 'group' (si group existe como verdad-terreno o cohorte)
tab_w <- table(real = df$group, clust = cl_w)
tab_c <- table(real = df$group, clust = cl_c)

tab_w
tab_c

	5.	Perfil de clusters (interpretación)

	•	Calcula promedios de cada nota por cluster para interpretar “tipos” de estudiantes.

# --- 5. Perfil de clusters ---
aggregate(df[, c("taller","quiz","parcial","midterm")], 
          by = list(cluster = cl_w), 
          FUN = mean)

# (opcional) Ordena estudiantes por su "altura" de fusión para ver casos límite
ord <- hc_ward$order
df[ord[1:10], c("codigo","taller","quiz","parcial","midterm")]

	6.	(Opcional) Sugerencia del número de clusters desde el dendrograma

	•	Observa los “saltos” grandes en hc_ward$height.

# --- 6. Heurística para elegir k (sin librerías) ---
plot(rev(hc_ward$height), type = "b",
     main = "Alturas de fusión (rev)",
     xlab = "Uniones (de última a primera)",
     ylab = "Altura")

